# -*- coding: utf-8 -*-
"""Data-Preparation-Visualisation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zrvZ0qFauiUO6cydg30BTwBCGwkD4AQm
"""

from google.colab import drive
drive.mount('/content/drive')

import os

os.environ['KAGGLE_USERNAME'] = "arkajyotichakraborty"

# username from the json file

os.environ['KAGGLE_KEY'] = "5ee317f1ac56552e105e16f46b81a8be"

!kaggle competitions download -c uw-madison-gi-tract-image-segmentation

!unzip '/content/uw-madison-gi-tract-image-segmentation.zip'

import numpy as np
import pandas as pd

import cv2

img = cv2.imread('/content/train/case101/case101_day20/scans/slice_0001_266_266_1.50_1.50.png')

from google.colab.patches import cv2_imshow

cv2_imshow(img)

img1 = cv2.imread('/content/train/case110/case110_day12/scans/slice_0001_360_310_1.50_1.50.png')
cv2_imshow(img1)

img2 = cv2.imread('/content/train/case43/case43_day0/scans/slice_0001_266_266_1.50_1.50.png')
cv2_imshow(img2)

import os
import torch
import numpy as np
import pandas as pd
import torch.nn as nn
from glob import glob
from PIL import Image
import matplotlib.pyplot as plt
from torchvision.utils import make_grid
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import ToTensor, Resize

# Data Preparation: 
# Defining the Paths:

train_dir = '/content/train'
train_meta_data = '/content/train.csv'

train_df = pd.read_csv(train_meta_data)
train_df

#PreProcessing : 

train_df["segmentation"] = train_df["segmentation"].astype("str")
train_df["case_id"] = train_df["id"].apply(lambda x: x.split("_")[0][4:])
train_df["day_id"] = train_df["id"].apply(lambda x: x.split("_")[1][3:])
train_df["slice_id"] = train_df["id"].apply(lambda x: x.split("_")[-1])

train_df.head()

from glob import glob

# Extracting the image paths from the imageIds

def fetch_file_from_id(root_dir, case_id):
    case_folder = case_id.split("_")[0]
    day_folder = "_".join(case_id.split("_")[:2])
    file_starter = "_".join(case_id.split("_")[2:])
    # fetching folder paths
    folder = os.path.join(root_dir, case_folder, day_folder, "scans")
    # fetching filenames with similar pattern
    file = glob(f"{folder}/{file_starter}*")
    # returning the first file, though it will always hold one file.
    return file[0]
train_df["path"] = train_df["id"].apply(lambda x: fetch_file_from_id(train_dir, x))
train_df.head()

train_df["height"] = train_df["path"].apply(lambda x: os.path.split(x)[-1].split("_")[2]).astype("int")
train_df["width"] = train_df["path"].apply(lambda x: os.path.split(x)[-1].split("_")[3]).astype("int")
train_df.head()

train_df["class"].unique()

# We will be label encoding the class variables.

class_names = train_df["class"].unique()
for index, label in enumerate(class_names):
    # replacing class names with indexes
    train_df["class"].replace(label, index, inplace = True)
train_df.head()

"""Mask Generation Methodology :
The Segmentation mask given in this place are in RLE (RUN LENGTH ENCODING) format.

At first we have to understand how the format is :
We can consider the given data has a particular shape , let's say height is H and width is W.
Now we can consider the data in a flattened manner i.e. shape of the data is (batch_size, H*W).
The mask values are seperated by a space where every pair of value holds the valid infomation.
The first value of the pair holds the sarting index of the mask in the flattened grid.
The second value of the pair holds the length of mask from that starter pixel.
So basically we can say , every element in the odd position holds the starter index and rest holds the length of the presence of the mask.
Now , to process the mask what we can do :
Split the odd elements and he even ones into different array which correspond length of present mask pixels and starter indexes.
Add all the indexes that must be masked.
Reshape the flattened array to image grid.
Adding utility functions to process mask and images
"""

def prepare_mask_data(string):
    # fetching all the values from the string
    all_values = map(int, string.split(" "))
    # preparing the usable arrays
    starterIndex, pixelCount = [], []
    for index, value in enumerate(all_values):
        if index % 2:
            # storing even indexed values in pixelCount
            pixelCount.append(value)
        else:
            # storing odd indexed values in starterIndex
            starterIndex.append(value)
    return starterIndex, pixelCount
    
def fetch_pos_pixel_indexes(indexes, counts):
    final_arr = []
    for index, counts in zip(indexes, counts):
        # adding all the values from starterIndex to range of positive pixel counts
        final_arr += [index + i for i in range(counts)]
    return final_arr

def prepare_mask(string, height, width):
    # preparing the respective arrays
    indexes, counts = prepare_mask_data(string)
    # preparing all the pixel indexes those have mask values
    pos_pixel_indexes = fetch_pos_pixel_indexes(indexes, counts)
    # forming the flattened array
    mask_array = np.zeros(height * width)
    # updating values in the array
    mask_array[pos_pixel_indexes] = 1
    # reshaping the masks
    return mask_array.reshape(height, width)

def load_image(path):
    # loading the image in RGB format
    image = Image.open(path).convert('RGB')
    return image

"""# DataSet Preparation"""

class UWDataset(Dataset):
    
    def __init__(self, meta_df, h=256, w=256):
        super().__init__()
        self.meta_df = meta_df
        self.h = h
        self.w = w
        self.resize = Resize((h, w))
        
    def __len__(self):
        return len(self.meta_df)
    
    def __getitem__(self, index):
        # fetching image path
        path = self.meta_df.loc[index, "path"]
        # loading image
        image = load_image(path)
        # loading mask's original height, width
        mask_h, mask_w = self.meta_df.loc[index, "height"], self.meta_df.loc[index, "width"]
        # loading the segmentation encoding for maks preparation
        mask_string = self.meta_df.loc[index, "segmentation"]
        # laoding the mask
        main_mask_channel = self.load_mask(string=mask_string, h=mask_h, w=mask_w)
        # updating those in tensor format
        image = ToTensor()(self.resize(image))
        main_mask_channel = ToTensor()(self.resize(main_mask_channel))
        # loading the original mask
        mask = torch.zeros((3, self.h, self.w))
        # loading the class label
        class_label = self.meta_df.loc[index, "class"]
        mask[class_label, ...] = main_mask_channel
        
        return image, mask
    
    def load_mask(self, string, h, w):
        # cheking if the segmentation encoding is a valid mask or null values
        if string != "nan":
            return Image.fromarray(prepare_mask(string, h, w))
        return Image.fromarray(np.zeros((h, w)))

#Parameters
SEED  = 42
BATCH_SIZE = 64
import warnings
warnings.filterwarnings("ignore")

ds = UWDataset(train_df)
print(f"Length of the dataset : {len(ds)}") #length of the dataset

image, mask = ds[194]
image.shape, mask.shape

combined_im_mask = torch.cat([image, mask], dim=2)

def show_image(tensor_image, name):
    plt.figure(figsize=(20, 20))
    plt.imshow(tensor_image.permute(1,2,0))
    plt.title(name, size=30)
    plt.show()
show_image(combined_im_mask, "Real & Mask")

"""Train & Validation Split"""

train_size = int(len(ds)*0.8)
val_size = len(ds) - train_size
train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size], generator=torch.Generator().manual_seed(42))
print(f"Length of the training dataset : {len(train_ds)}")
print(f"Length of the validation dataset : {len(val_ds)}")

train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle = True, drop_last = True)
val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last = True)

for train_image_batch, train_mask_batch in train_dl:
    break
for val_image_batch, val_mask_batch in val_dl:
    break

train_batch = torch.cat([make_grid(train_image_batch, nrow=8), make_grid(train_mask_batch, nrow=8)], dim=2)
val_batch = torch.cat([make_grid(val_image_batch, nrow=8), make_grid(val_mask_batch, nrow=8)], dim=2)

show_image(train_batch, "Training Batch")

show_image(val_batch, "Validation Batch")